import pandas as pd
import numpy as np

def calculate_rsrc(data):
    """
    Calculates the Resource-Scaled Reasoning Capability (RSRC) for AI models.

    This function computes the RSRC score based on a simplified formula,
    considering model parameters, training FLOPs, energy efficiency, and architecture type (MoE or Dense).
    It also incorporates adjustments for MoE models, older dense models, and applies a soft FLOPs cap.

    Args:
        data (dict): A dictionary containing AI model data.
                     Required keys for each model entry are:
                       - "Model" (str): Model name.
                       - "Architecture" (str): "MoE" or "Dense".
                       - "Parameters (B)" (float): Total parameters in billions.
                       - "Layers" (int, optional): Number of layers (can be None if unknown).
                       - "Training FLOPs" (float): Training FLOPs in petaFLOPs.
                       - "MMLU (5-shot)" (float): MMLU (5-shot) score in percentage.
                       - "Energy/pFLOP" (float): Energy per FLOP in pJ.
                       - "Year" (int): Year of model training or release.

    Returns:
        pandas.DataFrame: DataFrame with the input data and an added 'New RSRC' column
                          containing the calculated RSRC scores.
                          Returns NaN RSRC for any model where calculation is not possible
                          (e.g., due to division by zero).

    Formulas Used:
        FLOPs Efficiency Score = (Training FLOPs / Parameters^alpha) * (Energy/pFLOP / beta)
        RSRC = ln(Active Parameters) / (FLOPs Efficiency Score ^ gamma) * Age Decay

    Parameters and Adjustments:
        alpha (float): Parameter exponent in FLOPs Efficiency Score (fixed at 0.5).
        beta (float): Energy normalization factor (fixed at 50 pJ).
        gamma (float): Exponent for FLOPs Efficiency Score in RSRC formula;
                       0.48 for MoE, 0.45 for Dense (adjusts penalty for FLOPs inefficiency).
        moe_param_scale_factor (float): Scaling factor for MoE model parameters (fixed at 0.8).
        legacy_penalty_factor (float): Penalty factor for older dense models (fixed at 1.2).
        Age Decay:  Reduces RSRC for older models to reflect increased efficiency of newer models.
        Soft FLOPs Cap: Limits Training FLOPs to prevent overly large values from skewing RSRC.
        MoE Efficiency Cap: Scales down effective parameters for MoE models.
    """

    df = pd.DataFrame(data)

    # --- 1. Define Fixed Parameters ---
    alpha = 0.5
    beta = 50
    moe_param_scale_factor = 0.8
    legacy_penalty_factor = 1.2
    moe_exponent = 0.48
    dense_exponent = 0.45
    current_year = 2024 # You can adjust the current year for age decay calculations

    # --- 2. Apply Age Decay ---
    df["Age Decay"] = 1 - ((current_year - df["Year"]) * 0.05)
    df["Age Decay"] = df["Age Decay"].clip(lower=0.8)

    # --- 3. Calculate FLOPs Efficiency Score ---
    df["FLOPs Efficiency Score"] = (
        (df["Training FLOPs"] / (df["Parameters (B)"] ** alpha)) *
        (df["Energy/pFLOP"] / beta)
    )

    # --- 4. Apply Legacy Penalty to Older Dense Models ---
    df.loc[(df["Architecture"] == "Dense") & (df["Year"] < current_year), "FLOPs Efficiency Score"] *= legacy_penalty_factor

    # --- 5. Apply Soft FLOPs Cap ---
    df["Training FLOPs"] = np.minimum(df["Training FLOPs"], df["Parameters (B)"] ** 0.8 * 1e3)

    # --- 6. Apply MoE Efficiency Cap (Scale Active Parameters) ---
    df["Active Parameters"] = df["Parameters (B)"].copy()
    df.loc[df["Architecture"] == "MoE", "Active Parameters"] *= moe_param_scale_factor

    # --- 7. Manually Correct Active Parameters for Specific MoE Models (if needed) ---
    # Example corrections -  uncomment and modify as needed for specific models
    # df.loc[df["Model"] == "Specific-MoE-Model-1", "Active Parameters"] = Corrected_Active_Parameter_Value
    # df.loc[df["Model"] == "Specific-MoE-Model-2", "Active Parameters"] = Corrected_Active_Parameter_Value


    # --- 8. Calculate RSRC using Simplified Formula ---
    df["New RSRC"] = (np.log(df["Active Parameters"]) /
                      (df["FLOPs Efficiency Score"] ** df["Architecture"].apply(lambda x: moe_exponent if x == "MoE" else dense_exponent))) * df["Age Decay"]

    return df


if __name__ == '__main__':
    # --- Example Data Input ---
    # Replace this dictionary with your actual AI model data in the specified format.
    model_data = {
        "Model": [],  # List of model names (strings)
        "Architecture": [],  # List of architectures ("MoE" or "Dense" strings)
        "Parameters (B)": [],  # List of parameter counts in billions (floats)
        "Layers": [],  # List of layer counts (integers or None if unknown)
        "Training FLOPs": [],  # List of training FLOPs (floats)
        "MMLU (5-shot)": [],  # List of MMLU 5-shot scores (floats)
        "Energy/pFLOP": [],  # List of Energy/pFLOP values (floats)
        "Year": []   # List of training/release years (integers)
    }

    # --- Call the RSRC Calculation Function ---
    if model_data["Model"]: # Check if the model_data dictionary is populated
        rsrc_leaderboard_df = calculate_rsrc(model_data)

        # --- Output the Leaderboard ---
        # You can choose to print to console, save to CSV, or display in a table format.
        print(rsrc_leaderboard_df) # Prints the DataFrame to console

        # Example: Save to CSV (uncomment to use)
        # rsrc_leaderboard_df.to_csv("rsrc_leaderboard.csv", index=False)
        # print("\nLeaderboard saved to rsrc_leaderboard.csv")

    else:
        print("Error: No model data provided. Please populate the 'model_data' dictionary.")
